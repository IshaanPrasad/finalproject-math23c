rm(list=ls())
set.seed(123)
#Change this to location of your data
#Can use drop down menu in R studio: file->import data set-> from stata and find stata data set
setwd("/Users/abhishekmalani/Desktop/Math 23C/Data")
if (!require(foreign)) install.packages("foreign"); library(foreign)
if (!require(haven)) install.packages("haven"); library(haven)
census <- read.csv("finalproject.csv")
head(census)
index <- which(!is.na(census$Income) & !is.na(census$CensusTract) & !is.na(census$State)
& !is.na(census$County) & !is.na(census$TotalPop) & !is.na(census$Citizen)& !is.na(census$Men)
& !is.na(census$Women) & !is.na(census$Hispanic) & !is.na(census$White) & !is.na(census$Black)
& !is.na(census$Native) & !is.na(census$Asian) & !is.na(census$Pacific) & !is.na(census$IncomeErr)
& !is.na(census$IncomePerCap) & !is.na(census$IncomePerCapErr) & !is.na(census$Poverty)
& !is.na(census$ChildPoverty) & !is.na(census$Professional) & !is.na(census$Service)
& !is.na(census$Office) & !is.na(census$Construction) & !is.na(census$Production)
& !is.na(census$Drive) & !is.na(census$Carpool) & !is.na(census$Transit) & !is.na(census$Walk)
& !is.na(census$OtherTransp) & !is.na(census$WorkAtHome) & !is.na(census$MeanCommute)
& !is.na(census$Employed) & !is.na(census$PrivateWork) & !is.na(census$PublicWork)
& !is.na(census$SelfEmployed) & !is.na(census$FamilyWork) & !is.na(census$Unemployment)               )
clean <- census[index,]
head(clean)
write.csv(clean,"clean.csv")  #look at this in Excel or as a text file
median(clean$Income)
length(clean$Income)
head(clean)
median(clean$Income)
length(clean$Income)
hist(clean$Income)
h = hist(clean$Income, probability = TRUE) # or hist(x,plot=FALSE) to avoid the plot of the histogram
h$density = h$counts/sum(h$counts)
plot(h,freq=FALSE)
#To find the best fitting normal distribution, compute the mean and variance of the data
mu <- mean(clean$Income); mu
sigma <- sd(clean$Income)     #estimates square root of the population variance
curve(dnorm(x, mu, sigma), from = 0, to = 250000, add = TRUE, col = "red")
head(clean)
income = clean$Income
hispanic = clean$Hispanic
white = clean$White
black = clean$Black
native = clean$Native
asian = clean$Asian
pacific = clean$Pacific
mostlywhite = clean[which(white>80)]
mostlywhite = clean$Income[which(white>80)]
mostlywhite = clean$Income[which(white>80)]; mostlywhite
mostlyblack = clean$Income[which(black>80)]; mostlyblack
mostlyhispanic = clean$Income[which(hispanic>80)]
mostlyasian = clean$Income[which(asian>80)]
mostlywhite = clean$Income[which(white>80)]; mostlywhite; length(mostlywhite)
mostlyblack = clean$Income[which(black>80)];  length(mostlyblack)
mostlyhispanic = clean$Income[which(hispanic>80)];  length(mostlyhispanic)
mostlyasian = clean$Income[which(asian>80)]; length(mostlyasian)
mostlynative = clean$Income[which(native>80)]; length(mostlynative)
mostlypacific = clean$Income[which(pacific>80)]; length(mostlypacific)
mostlywhite = clean$Income[which(white>75)]; mostlywhite; length(mostlywhite)
mostlyblack = clean$Income[which(black>75)];  length(mostlyblack)
mostlyhispanic = clean$Income[which(hispanic>75)];  length(mostlyhispanic)
mostlyasian = clean$Income[which(asian>75)]; length(mostlyasian)
mostlynative = clean$Income[which(native>75)]; length(mostlynative)
mostlypacific = clean$Income[which(pacific>75)]; length(mostlypacific)
head(clean)
mean(clean$Asian)
mostlyasian = clean$Income[which(asian>50)]; length(mostlyasian)
whiteincome = mean(mostlywhite)
blackincome = mean(mostlyblack)
hispanicincome = mean(mostlyhispanic)
asianincome = mean(mostlyasian)
nativeincome = mean(mostlynative)
mostlypacific = clean$Income[which(pacific>50)]; length(mostlypacific)
mostlypacific = clean$Income[which(pacific>75)]; length(mostlypacific)
pacificincome = mean(mostlypacific)
whiteincome; blackincome;hispanicincome;asianincome;nativeincome;pacificincome
plot(whiteincome; blackincome;hispanicincome;asianincome;nativeincome;pacificincome)
plot(whiteincome, blackincome, hispanicincome, asianincome, nativeincome, pacificincome)
barplot(whiteincome, blackincome, hispanicincome, asianincome, nativeincome, pacificincome)
?barplot
barplot(c(whiteincome, blackincome, hispanicincome, asianincome, nativeincome, pacificincome))
barplot(c(whiteincome, blackincome, hispanicincome, asianincome, nativeincome, pacificincome), names.arg = c("white", "black", "hispanic", "asian", "native", "pacific"))
barplot(c(whiteincome, blackincome, hispanicincome, asianincome, nativeincome, pacificincome), names.arg = c("white", "black", "hispanic", "asian", "native", "pacific"), col =c("white", "black","brown", "yellow", "red", "blue"))
barplot(c(whiteincome, blackincome, hispanicincome, asianincome, nativeincome, pacificincome), names.arg = c("white", "black", "hispanic", "asian", "native", "pacific"), col =c("white", "black","brown", "yellow", "red", "tan"))
barplot(c(whiteincome, blackincome, hispanicincome, asianincome, nativeincome, pacificincome), names.arg = c("white", "black", "hispanic", "asian", "native", "pacific"), col =c("white", "black","brown", "yellow", "red", "tan"),main = "Income")
barplot(c(whiteincome, blackincome, hispanicincome, asianincome, nativeincome, pacificincome), names.arg = c("white", "black", "hispanic", "asian", "native", "pacific"),main = "Income")
plot(c(0,0), (1,200000))
c
plot(c(0,0), c(1,200000))
scatter.smooth(c(0,0), c(1,200000))
scatter.smooth(c(0,0), c(1,200000))
scatter.smooth(c(0,1), c(0,200000))
plot(c(0,1), c(0,200000))
hispanic = which(clean$Hispanic>75)
white = which(clean$White>75)
black = which(clean$Black>75)
native = which(clean$Native>75)
asian = which(clean$Asian>75)
pacific = which(clean$Pacific>75)
mostlywhite = clean$Income[white]; mostlywhite; length(mostlywhite)
mostlyblack = clean$Income[black];  length(mostlyblack)
mostlyhispanic = clean$Income[hispanic];  length(mostlyhispanic)
mostlyasian = clean$Income[asian]; length(mostlyasian)
mostlynative = clean$Income[native]; length(mostlynative)
mostlypacific = clean$Income[pacific]; length(mostlypacific)
whiteincome = mean(mostlywhite)
blackincome = mean(mostlyblack)
hispanicincome = mean(mostlyhispanic)
asianincome = mean(mostlyasian)
nativeincome = mean(mostlynative)
pacificincome = mean(mostlypacific)
whiteincome; blackincome;hispanicincome;asianincome;nativeincome;pacificincome
barplot(c(whiteincome, blackincome, hispanicincome, asianincome, nativeincome, pacificincome), names.arg = c("white", "black", "hispanic", "asian", "native", "pacific"),main = "Income")
hispanic = which(clean$Hispanic>75);  length(mostlyhispanic)
white = which(clean$White>75); length(mostlywhite)
black = which(clean$Black>75);  length(mostlyblack)
native = which(clean$Native>75); length(mostlynative)
asian = which(clean$Asian>75); length(mostlyasian)
pacific = which(clean$Pacific>75); length(mostlypacific)
whitepoverty = mean(mostlywhite)
blackpoverty = mean(mostlyblack)
hispanicpoverty = mean(mostlyhispanic)
asianpoverty = mean(mostlyasian)
nativepoverty = mean(mostlynative)
pacificpoverty = mean(mostlypacific)
whitepoverty; blackpoverty;hispanicpoverty;asianpoverty;nativepoverty;pacificpoverty
mostlywhite = clean$Poverty[white]
mostlyblack = clean$Poverty[black]
mostlyhispanic = clean$Poverty[hispanic]
mostlyasian = clean$Poverty[asian]
mostlynative = clean$Poverty[native]
mostlypacific = clean$Poverty[pacific]
whitepoverty = mean(mostlywhite)
blackpoverty = mean(mostlyblack)
hispanicpoverty = mean(mostlyhispanic)
asianpoverty = mean(mostlyasian)
nativepoverty = mean(mostlynative)
pacificpoverty = mean(mostlypacific)
whitepoverty; blackpoverty;hispanicpoverty;asianpoverty;nativepoverty;pacificpoverty
barplot(c(whitepoverty,blackpoverty,hispanicpoverty,asianpoverty,nativepoverty,pacificpoverty), names.arg = c("white", "black", "hispanic", "asian", "native", "pacific"),main = "Income")
barplot(c(whitepoverty,blackpoverty,hispanicpoverty,asianpoverty,nativepoverty,pacificpoverty), names.arg = c("white", "black", "hispanic", "asian", "native", "pacific"),main = "Poverty")
rm(list=ls())
set.seed(123)
#Change this to location of your data
setwd("/Users/abhishekmalani/Desktop/Math 23C/finalproject-math23c")
?setwd
getwd()
setwd("/Users/abhishekmalani/Desktop/Math 23C/Data")
setwd("Users/abhishekmalani/Desktop/Math 23C/Data")
#Change this to location of your data
#Can use drop down menu in R studio: file->import data set-> from stata and find stata data set
setwd("/Users/Abhishekmalani/Desktop/Math 23C/Data")
#Change this to location of your data
setwd(/Users/abhishekmalani/Desktop/Math 23C/finalproject-math23c)
getwd()
#Change this to location of your data
#Can use drop down menu in R studio: file->import data set-> from stata and find stata data set
setwd("/Users/Massimo/Documents/Math 23c/Data")
#Change this to location of your data
#Can use drop down menu in R studio: file->import data set-> from stata and find stata data set
setwd("/Users/abishekmalani/Documents/Math 23c")
#Change this to location of your data
#Can use drop down menu in R studio: file->import data set-> from stata and find stata data set
setwd("/abishekmalani/Documents/Math 23c")
#Topic 4: Logistic regression
SOX <- read.csv("RedSox2013.csv"); head(SOX)
#Convert the WonLost column to a Bernoulli random variable
wins <- (as.numeric(SOX$WonLost=="W")); head(wins)
#Extract the runs scored column
runs <- SOX$R
plot(runs,wins)  #not a great candidate for a straight-line approximation, but let's try
b <- cov(runs,wins)/var(runs)    #easy way to get the slope
#Here is the formula for the intercept
a <- mean(wins) - b*mean(runs);a
#We can add this regression line to the plot of the data
abline(a, b, col = "red")
#Instead, we assume that p = exp(alpha x+beta)/(1 + exp(alpha x+beta))
#This function can never be less than zero nor greater than 1
#Start with minus the log of the likelihood function
MLL<- function(alpha, beta) {
-sum( log( exp(alpha+beta*runs)/(1+exp(alpha+beta*runs)) )*wins
+ log(1/(1+exp(alpha+beta*runs)))*(1-wins) )
}
#R has a function that will maximize this function of alpha and beta
#install.packages("stats4")   #needs to be run at most once
library(stats4)
results<-mle(MLL, start = list(alpha = 0, beta = 0)) #an initial guess is required
results@coef
curve( exp(results@coef[1]+results@coef[2]*x)/ (1+exp(results@coef[1]+results@coef[2]*x)),col = "blue", add=TRUE)
#The curve shows a good model for the probability of winning as a function of runs
abline(h=0.5)
abline(v=c(3,4))
index <- which(runs == 3); head(index)   #games with 3 runs
mean(wins[index])   #won 34%
index <- which(runs == 4)    #games with 4 runs
mean(wins[index])   #won 63%
index <- which(runs == 7)    #games with 7 runs
mean(wins[index])   #won 88%
index <- which(runs == 9)    #games with 9 runs
mean(wins[index])   #won 100%
if (!require(stats4)) install.packages("stats4"); library(stats4)
barplot(c(whiteincome, blackincome, hispanicincome, asianincome, nativeincome, pacificincome), names.arg = c("white", "black", "hispanic", "asian", "native", "pacific"),main = "Income")
clean = read.csv("clean.csv")
rm(list=ls())
set.seed(123)
#Change this to location of your data
#Can use drop down menu in R studio: file->import data set-> from stata and find stata data set
setwd("/Users/abhishekmalani/Desktop/Math 23C/Data")
if (!require(foreign)) install.packages("foreign"); library(foreign)
if (!require(haven)) install.packages("haven"); library(haven)
if (!require(randomForest)) install.packages("randomForest"); library(randomForest)
if (!require(rpart)) install.packages("rpart"); library(rpart)
if (!require(stats4)) install.packages("stats4"); library(stats4)
census <- read.csv("finalproject.csv")
census <- read.csv("finalproject.csv")
census <- read.csv("finalproject.csv")
rm(list=ls())
set.seed(123)
#Change this to location of your data
#Can use drop down menu in R studio: file->import data set-> from stata and find stata data set
setwd("/Users/abhishekmalani/Desktop/Math 23C/Data")
if (!require(foreign)) install.packages("foreign"); library(foreign)
if (!require(haven)) install.packages("haven"); library(haven)
if (!require(randomForest)) install.packages("randomForest"); library(randomForest)
if (!require(rpart)) install.packages("rpart"); library(rpart)
if (!require(stats4)) install.packages("stats4"); library(stats4)
census <- read.csv("finalproject.csv")
clean = read.csv("clean.csv")
rm(list=ls())
set.seed(123)
if (!require(foreign)) install.packages("foreign"); library(foreign)
if (!require(haven)) install.packages("haven"); library(haven)
if (!require(randomForest)) install.packages("randomForest"); library(randomForest)
if (!require(rpart)) install.packages("rpart"); library(rpart)
census <- read.csv("finalproject.csv")
#Change this to location of your data
setwd("/Users/Massimo/Documents/Math 23c/finalproject-math23c")
census <- read.csv("finalproject.csv")
head(census)
index <- which(!is.na(census$Income) & !is.na(census$CensusTract) & !is.na(census$State)
& !is.na(census$County) & !is.na(census$TotalPop) & !is.na(census$Citizen)& !is.na(census$Men)
& !is.na(census$Women) & !is.na(census$Hispanic) & !is.na(census$White) & !is.na(census$Black)
& !is.na(census$Native) & !is.na(census$Asian) & !is.na(census$Pacific) & !is.na(census$IncomeErr)
& !is.na(census$IncomePerCap) & !is.na(census$IncomePerCapErr) & !is.na(census$Poverty)
& !is.na(census$ChildPoverty) & !is.na(census$Professional) & !is.na(census$Service)
& !is.na(census$Office) & !is.na(census$Construction) & !is.na(census$Production)
& !is.na(census$Drive) & !is.na(census$Carpool) & !is.na(census$Transit) & !is.na(census$Walk)
& !is.na(census$OtherTransp) & !is.na(census$WorkAtHome) & !is.na(census$MeanCommute)
& !is.na(census$Employed) & !is.na(census$PrivateWork) & !is.na(census$PublicWork)
& !is.na(census$SelfEmployed) & !is.na(census$FamilyWork) & !is.na(census$Unemployment)               )
clean <- census[index,]
head(clean)
write.csv(clean,"clean.csv")  #look at this in Excel or as a text file
#Median income across all census tracts
median(clean$Income)
#Histogram with Normal Distribution added
hist(clean$Income)
h = hist(clean$Income)
h$density = h$counts/sum(h$counts)
plot(h,freq=FALSE)
hist(clean$Income, breaks = "FD", probability = TRUE)
#To find the best fitting normal distribution, compute the mean and variance of the data
mu <- mean(clean$Income); mu
var <- var(clean$Income)
sigma <- sd(clean$Income)     #estimates square root of the population variance
curve(dnorm(x, mu, sigma), from = 0, to = 250000, add = TRUE, col = "red")
#Visualizng the percentage of a race in a population w.r.t. the average income of the population
percentA <- clean$Asian / 100       # Asian
percentB <- clean$Black / 100       # Black
percentH <- clean$Hispanic / 100    # Hispanic
percentN <- clean$Native / 100      # Native
percentP <- clean$Pacific / 100     # Pacific
percentW <- clean$White / 100       # White
income <-  clean$Income
# ggplotting the data: fulfilling "Nicely labeled graphics using ggplot, with good use of color, line styles, etc., that tell a convincing story"
df <- data.frame(
percentA,      # Asian
percentB,      # Black
percentH,   # Hispanic
percentN,    # Native
percentP,  # Pacific
percentW,       # White
income
)
# Asian
ggplot(df, aes(percentA, income)) +
geom_point(color = "red", size = 0.5) +
geom_smooth(method = "lm", color ="blue") +
xlab("Percent of Asian People in Tract") +
ylab("Average Income of Tract")
# Black
ggplot(df, aes(percentB, income)) +
geom_point(color = "red", size = 0.5) +
geom_smooth(method = "lm", color ="blue") +
xlab("Percent of Black People in Tract") +
ylab("Average Income of Tract")
# Hispanic
ggplot(df, aes(percentH, income)) +
geom_point(color = "red", size = 0.5) +
geom_smooth(method = "lm", color ="blue") +
xlab("Percent of Hispanic People in Tract") +
ylab("Average Income of Tract")
# Native
ggplot(df, aes(percentN, income)) +
geom_point(color = "red", size = 0.5) +
geom_smooth(method = "lm", color ="blue") +
xlab("Percent of Native People in Tract") +
ylab("Average Income of Tract")
# Pacific
ggplot(df, aes(percentP, income)) +
geom_point(color = "red", size = 0.5) +
geom_smooth(method = "lm", color ="blue") +
xlab("Percent of Pacific People in Tract") +
ylab("Average Income of Tract")
# White
ggplot(df, aes(percentW, income)) +
geom_point(color = "red", size = 0.5) +
geom_smooth(method = "lm", color ="blue") +
xlab("Percent of White People in Tract") +
ylab("Average Income of Tract")
# Linear regression on the above scatter plots
Percent <- c(0,1)
Income <- c(0, 200000)
plot(Percent, Income, pch=".") # To clear plot space but keep axes in tact
abline(lm(income ~ percentH), col = "red") # Hispanic
abline(lm(income ~ percentW), col = "orange") # White
abline(lm(income ~ percentB), col = "yellow") # Black
abline(lm(income ~ percentN), col = "green") # Native
abline(lm(income ~ percentA), col = "blue") # Asian
abline(lm(income ~ percentP), col = "violet") # Pacific
sum(clean$State == "Massachusetts")
IlAvg <- sum(clean$Transit*(clean$State == "Illinois"))/sum(clean$State == "Illinois"); IlAvg
MAAvg <- sum(clean$Transit*(clean$State == "Massachusetts"))/sum(clean$State == "Massachusetts"); MAAvg
observed <- MAAvg - IlAvg; observed
#Now replace all the states with a random sample of all the data
State <- sample(clean$State); State   #permuted state column
sum(State == "Massachusetts")  #still 1453 but each will match up with random transit percentage
IlAvg <- sum(clean$Transit*(State == "Illinois"))/sum(State == "Illinois"); IlAvg
MAAvg <- sum(clean$Transit*(State == "Massachusetts"))/sum(State == "Massachusetts"); MAAvg
MAAvg - IlAvg   #as likely to be negative or positive
#Repeat 10000 times
N <- 10000
diffs <- numeric(N)
for (i in 1:N){
State <- sample(clean$State); State  #permuted State column
IlAvg <- sum(clean$Transit*(State == "Illinois"))/sum(State == "Illinois"); IlAvg
MAAvg <- sum(clean$Transit*(State == "Massachusetts"))/sum(State == "Massachusetts"); MAAvg
diffs[i] <- MAAvg - IlAvg    #as likely to be negative or positive
}
mean(diffs) #should be close to zero
hist(diffs, breaks = "FD")
#Now display the observed difference on the histogram
abline(v = observed, col = "red")
#What is the probability (the P value) that a difference this large
#could have arisen with a random subset?
pvalue <- 2*(1- (sum(diffs >= observed)+1)/(N+1)); pvalue #two sided significance test
Chicago <- data.frame(clean$CensusTract, clean$County, clean$State, clean$Transit)
ChicagoConditional <- (clean$County == "Cook" & clean$State == "Illinois")
Chicago <- Chicago[ChicagoConditional,]
Chicago <- Chicago[sample(nrow(Chicago), 191), ]
Boston <- data.frame(clean$CensusTract, clean$County, clean$State, clean$Transit)
BostonConditional <- (clean$County == "Suffolk" & clean$State == "Massachusetts")
Boston <- Boston[BostonConditional,]
Overall <- rbind(Chicago,Boston)
ChicagoAvg <- sum(Overall$clean.Transit*(Overall$clean.County == "Cook"))/sum(Overall$clean.County == "Cook"); ChicagoAvg
BostonAvg <- sum(Overall$clean.Transit*(Overall$clean.County == "Suffolk"))/sum(Overall$clean.County == "Suffolk"); BostonAvg
observed <- BostonAvg - ChicagoAvg; observed
#Now replace Massachusetts with a random sample of all the data
County <- sample(Overall$clean.County); County   #permuted State column
sum(County == "Cook")  #still 15 men but they will match up with random beer consumption
ChicagoAvg <- sum(Overall$clean.Transit*(County == "Cook"))/sum(County == "Cook"); ChicagoAvg
BostonAvg <- sum(Overall$clean.Transit*(County == "Suffolk"))/sum(County == "Suffolk"); BostonAvg
BostonAvg - ChicagoAvg  #as likely to be negative or positive
#Repeat 10000 times
N <- 10000
diffs <- numeric(N)
for (i in 1:N){
Chicago <- data.frame(clean$CensusTract, clean$County, clean$State, clean$Transit)
ChicagoConditional <- (clean$County == "Cook" & clean$State == "Illinois")
Chicago <- Chicago[ChicagoConditional,]
Chicago <- Chicago[sample(nrow(Chicago), 191), ]
Boston <- data.frame(clean$CensusTract, clean$County, clean$State, clean$Transit)
BostonConditional <- (clean$County == "Suffolk" & clean$State == "Massachusetts")
Boston <- Boston[BostonConditional,]
Overall <- rbind(Chicago,Boston)
County <- sample(Overall$clean.County); County   #permuted State column
ChicagoAvg <- sum(Overall$clean.Transit*(County == "Cook"))/sum(County == "Cook"); ChicagoAvg
BostonAvg <- sum(Overall$clean.Transit*(County == "Suffolk"))/sum(County == "Suffolk"); BostonAvg
diffs[i] <- BostonAvg - ChicagoAvg    #as likely to be negative or positive
}
mean(diffs) #should be close to zero
hist(diffs, xlim=c(-6,13), breaks = "FD")
#Now display the observed difference on the histogram
abline(v = observed, col = "red")
#What is the probability (the P value) that a difference this large
#could have arisen with a random subset?
pvalue <- 2*(sum(diffs >= observed)+1)/(N+1); pvalue #two sided test
#Storing predictor variables
vars <- colnames(clean[18:ncol(clean)])
vars
#Randomly sampling 70% of the data for a training set and the other 30% is testing
insample <- c(rep(T, 50908),rep(F,21819))
clean2 <- data.frame(clean, insample); head(clean2)
nrow(clean2)
insample <- sample(clean2$insample) #permuting to randomize which data points are selected for in-sample vs out-of-sample
clean2 <- data.frame(clean, insample); head(clean2)
#OLS Regression
to_hat <- with(clean2[clean2$insample==1,], lm(reformulate(vars, "Income")))
summary(to_hat)
rank_hat_ols = predict(to_hat, newdata=clean2)
summary(rank_hat_ols)
#Decision Tree or Regression Tree
one_tree <- rpart(reformulate(vars, "Income")
, data=clean2
, subset = clean2$insample==1
, control = rpart.control(xval = 5)) ## this sets the number of folds for cross validation.
one_tree #Text Representation of Tree
#Decision tree shows that the best predictors
rank_hat_tree <- predict(one_tree, newdata=clean2)
table(rank_hat_tree)
hist(rank_hat_tree, xlab="Predicted Rates - Single Tree")
plot(one_tree) # plot tree
text(one_tree) # add labels to tree, download as PDF to see full version (looks cramped on screen)
# print complexity parameter table using cross validation
printcp(one_tree)
# #Random Forest from 500 Bootstrapped Samples
forest_hat <- randomForest(reformulate(vars, "Income"), ntree=500, mtry=20, maxnodes=50
,importance=TRUE, do.trace=25, data=clean2[clean2$insample==1,])
getTree(forest_hat, 100, labelVar = TRUE) #Text Representation of Tree
rank_hat_forest <- predict(forest_hat, newdata=clean2,type="response")
summary(rank_hat_forest); hist(rank_hat_forest, xlab="Predicted Rates - Random Forest")
#Out-of-Sample Validation
if (!require(stargazer)) install.packages("stargazer"); library(stargazer)
#Merge training data with test data
clean2$rank_hat_ols <- rank_hat_ols
clean2$rank_hat_tree <- rank_hat_tree
clean2$rank_hat_forest <- rank_hat_forest
predictions <- clean2[clean2$insample==0,]
predictions <- predictions[,-c(4:13)]
predictions <- predictions[,-c(5:7)]
predictions <- predictions[,-c(5:25)]
predictions$ols_error <- predictions$Income - predictions$rank_hat_ols
predictions$tree_error <- predictions$Income - predictions$rank_hat_tree
predictions$forest_error <- predictions$Income - predictions$rank_hat_forest
mean(predictions$ols_error)^2
mean(predictions$tree_error)^2
mean(predictions$forest_error)^2
write.csv(predictions, "predictions.csv") #Save data as an excel .csv file
